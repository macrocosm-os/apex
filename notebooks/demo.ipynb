{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-06 15:42:57.988\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mProject version: 2.7.0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/prompting/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-06 15:43:00,417\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\u001b[32m2024-09-06 15:43:00.801\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.llms.utils\u001b[0m:\u001b[36mcalculate_single_gpu_requirements\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mAvailable free memory: 84.65 GB\u001b[0m\n",
      "\u001b[32m2024-09-06 15:43:00.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.llms.utils\u001b[0m:\u001b[36mcalculate_single_gpu_requirements\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mTotal gpu memory 85.1 GB\u001b[0m\n",
      "\u001b[32m2024-09-06 15:43:00.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.llms.utils\u001b[0m:\u001b[36mcalculate_single_gpu_requirements\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m24.0% of the GPU memory will be utilized for loading the model to device \"CUDA\".\u001b[0m\n",
      "\u001b[32m2024-09-06 15:43:00.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.llms.vllm_llm\u001b[0m:\u001b[36mload_vllm_pipeline\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mLoading VLLM pipeline with model_id casperhansen/llama-3-8b-instruct-awq: Max. VRAM: 0.24; GPUs: 1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-06 15:43:00 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 09-06 15:43:00 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/llama-3-8b-instruct-awq', speculative_config=None, tokenizer='casperhansen/llama-3-8b-instruct-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/llama-3-8b-instruct-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 09-06 15:43:01 model_runner.py:680] Starting to load model casperhansen/llama-3-8b-instruct-awq...\n",
      "INFO 09-06 15:43:01 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    }
   ],
   "source": [
    "# Initialise the LLM we use on the validator\n",
    "from prompting.llms.vllm_llm import vLLMPipeline\n",
    "pipeline = vLLMPipeline(llm_model_id=\"casperhansen/llama-3-8b-instruct-awq\", llm_max_allowed_memory_in_gb=20, device=\"CUDA\", quantization=False, llm_max_model_len=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.15s/it, est. speed input: 1.69 toks/s, output: 61.67 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' * 1+2 is 3.\\n\\n* What\\'s 1+3? * 1+3 is 4.\\n\\n* What\\'s 2+2? * 2+2 is 4.\\n\\n* What\\'s 3+3? * 3+3 is 6.\\n\\n* What\\'s 4+4? * 4+4 is 8.\\n\\nAs you can see, the numbers are getting bigger and bigger, and the answers are getting bigger and bigger too!\\n\\nCan you think of any other questions that would be similar to these ones?\\n\\nChallenge: Think of a different type of question that would have the same pattern as these ones.\\n\\n(Note: You can use symbols, letters, or any other type of input you like!)\\n\\nThink about it for a little bit... and then share your answer with me!  I\\'d love to see what you come up with!\\n\\nHappy thinking! ðŸ¤”\\n\\n(And don\\'t worry if you need a hint or two â€“ I\\'ll be here to help you out!) ðŸŽ‰\" | By | Date | Time | Date | Time | Date | Time | Date | Time | Date | Time | Date | Time | Date | Time | Date | Time | Date | Time | Date | Time |'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as you can see, \"pipeline\" is an object that simply wraps around the LLM and is callable\n",
    "pipeline(\"What's 1+2?\").split(\"\\n\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Datasets generate 'Context' objects, which contain a 'row' of data, in this case about wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/pydantic/_internal/_fields.py:201: UserWarning: Field name \"name\" in \"WikiDateDataset\" shadows an attribute in parent \"BaseDataset\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Context(title='Iqbal Bhatkal', topic='Terrorism', subtopic=\"He was involved in the Germany bakery blast, which resulted in the death of seventeen and injury of sixty-four people.[4][1] According to Pune Police, Mohsin Chaudhari, an accused for the German Bakery blast was recruited by Iqbal Shabandari, while Iqbal was living in Mumbai. Iqbal made friends with him during a religious function while he was in Pune. Iqbal kept in touch with him constantly for the next three years. With the help of Mohsin and Iqbal's other associates, they were able to set up an Indian Mujahideen base in Pune.[5]  Iqbal Shabandari had started his career as a fervent proselytizer for the largely apolitical Tablighi Jamaat and later on became an Islamist ideologue who recruited several Pune men into the Indian Mujahideen. While the two Bhatkal brothers were living in a rented apartment in Khondwa, Pune they presided over terrorist networks that later carried out multiple bombings across the country.[6]\\n\", content=\"He was involved in the Germany bakery blast, which resulted in the death of seventeen and injury of sixty-four people.[4][1] According to Pune Police, Mohsin Chaudhari, an accused for the German Bakery blast was recruited by Iqbal Shabandari, while Iqbal was living in Mumbai. Iqbal made friends with him during a religious function while he was in Pune. Iqbal kept in touch with him constantly for the next three years. With the help of Mohsin and Iqbal's other associates, they were able to set up an Indian Mujahideen base in Pune.[5]  Iqbal Shabandari had started his career as a fervent proselytizer for the largely apolitical Tablighi Jamaat and later on became an Islamist ideologue who recruited several Pune men into the Indian Mujahideen. While the two Bhatkal brothers were living in a rented apartment in Khondwa, Pune they presided over terrorist networks that later carried out multiple bombings across the country.[6]\\n\", internal_links=['Early life', 'Terrorism'], external_links=['Indian Mujahideen', 'Riyaz Bhatkal', 'Mahmud of Ghazni', 'Muhammad of Ghor', 'Central Bureau of Investigation', 'The Times of India', '2007 Uttar Pradesh bombings', '2010 Pune bombing', 'Batla House encounter case', 'Bhatkal'], source='Wikipedia', tags=['1970 births', 'Anti-Hindu sentiment', 'Fugitives wanted by India', 'Fugitives wanted on terrorism charges', 'Indian Islamists', 'Indian Mujahideen members', 'Indian expatriates in Pakistan', 'Leaders of Islamic terror groups', 'Living people', 'People charged with terrorism', 'People from Uttara Kannada', 'Short description matches Wikidata', 'Tablighi Jamaat people', 'Use Indian English from January 2020', 'Use dmy dates from January 2020', 'Webarchive template wayback links'], extra={'url': 'https://en.wikipedia.org/wiki/Iqbal_Bhatkal', 'page_length': 565, 'section_length': 151}, stats=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prompting.datasets.wiki import WikiDataset\n",
    "dataset = WikiDataset()\n",
    "context = dataset.random()\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Tasks are objects that can be used to generate the query & reference for a miner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise with past data\n",
    "\n",
    "We can either initialise the task with past data (this doesn't require an LLM to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bittensor: - ðŸ¤– Generating query... - \n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.12s/it, est. speed input: 84.95 toks/s, output: 56.33 toks/s]\n",
      "INFO:bittensor: - vLLM_LLM generated the following output:\n",
      "\n",
      "\n",
      "Here's a summary question:\n",
      "\n",
      "What is the significance of Iqbal Bhatkal, the founder of the terrorist organization Indian Mujahideen (IM), in Indian history, and how have his actions impacted the country's security landscape?\n",
      "\n",
      "(Note: The answer should be around 150-200 words) - \n",
      "INFO:bittensor: - ðŸ¤– Generating reference... - \n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.43s/it, est. speed input: 124.25 toks/s, output: 60.07 toks/s]\n",
      "INFO:bittensor: - vLLM_LLM generated the following output:\n",
      "\n",
      "\n",
      "Here is a concise and accurate summary of the context:\n",
      "\n",
      "Iqbal Shabandari, an Islamist ideologue, was involved in recruiting individuals for the Indian Mujahideen, a terrorist organization. He started by proselytizing for the Tablighi Jamaat, a religious group, and later became a recruiter for the Indian Mujahideen. Iqbal had connections with Mohsin Chaudhari, an accused in the Germany bakery blast, which resulted in the deaths of 17 people and injuries to 64 others. The two men, along with others, set up an Indian Mujahideen base in Pune, and Iqbal's associates carried out multiple bombings across the country. - \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"\\n\\nHere's a summary question:\\n\\nWhat is the significance of Iqbal Bhatkal, the founder of the terrorist organization Indian Mujahideen (IM), in Indian history, and how have his actions impacted the country's security landscape?\\n\\n(Note: The answer should be around 150-200 words)\",\n",
       " \"\\n\\nHere is a concise and accurate summary of the context:\\n\\nIqbal Shabandari, an Islamist ideologue, was involved in recruiting individuals for the Indian Mujahideen, a terrorist organization. He started by proselytizing for the Tablighi Jamaat, a religious group, and later became a recruiter for the Indian Mujahideen. Iqbal had connections with Mohsin Chaudhari, an accused in the Germany bakery blast, which resulted in the deaths of 17 people and injuries to 64 others. The two men, along with others, set up an Indian Mujahideen base in Pune, and Iqbal's associates carried out multiple bombings across the country.\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prompting.tasks.summarization import SummarizationTask, SummarizationRewardConfig\n",
    "SummarizationTask.generate_query_reference(llm_pipeline=pipeline, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miner Responses\n",
    "\n",
    "Now let's say we have a few miners giving us responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/pydantic/_internal/_fields.py:201: UserWarning: Field name \"required_hash_fields\" in \"StreamPromptingSynapse\" shadows an attribute in parent \"StreamingSynapse\"\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from prompting.base.dendrite import DendriteResponseEvent, SynapseStreamResult, StreamPromptingSynapse\n",
    "\n",
    "miner_response_1 = SynapseStreamResult(synapse=StreamPromptingSynapse(completion=\"4\", roles=[\"user\"], messages=[\"What's 1+2?\"]))\n",
    "miner_response_2 = SynapseStreamResult(synapse=StreamPromptingSynapse(completion=\"3\", roles=[\"assistant\"], messages=[\"What's 1+2?\"]))\n",
    "\n",
    "\n",
    "# the synapses from all miners get collected into the DenriteResponseEvent\n",
    "dendrite_response = DendriteResponseEvent(stream_results=[miner_response_1, miner_response_2], uids=[1, 2], timeout=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring\n",
    "\n",
    "We can now pass the query, reference and miner responses to our scoring function, which is then responsible for giving each miner a score which is later used to set weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-05 11:10:37,316\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_name\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_type\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.34321602, 0.78916947])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prompting.tasks.summarization import SummarizationRewardConfig\n",
    "\n",
    "reward_events, penality_events, rewards = SummarizationRewardConfig.apply(challenge=\"What's 1+2?\", reference=\"1+2 is equal to 3\", response_event=dendrite_response)\n",
    "rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other tests/examples on different tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompting.tasks.qa import QuestionAnsweringTask, QARewardConfig\n",
    "qa = QuestionAnsweringTask(context=context, llm_pipeline=pipeline, reward_config=SummarizationRewardConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.generate_query(pipeline=pipeline)\n",
    "qa.generate_reference(pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.34it/s, est. speed input: 139.69 toks/s, output: 343.83 toks/s]\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.40it/s, est. speed input: 567.92 toks/s, output: 360.75 toks/s]\n"
     ]
    }
   ],
   "source": [
    "qa_task = QuestionAnsweringTask(llm_pipeline=pipeline, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{{ Answer the question you will receive in detail, utilizing the following context.\n",
      "\n",
      "# Context:\n",
      "\n",
      "History\n",
      "\n",
      "# Question:\n",
      "\n",
      "None\n",
      "\n",
      " }}<|eot_id|><|start_header_id|>presenter<|end_header_id|>\n",
      "\n",
      "{{ Answer the question you will receive in detail, utilizing the following context.\n",
      "\n",
      "# Context:\n",
      "\n",
      "History\n",
      "\n",
      "# Question:\n",
      "\n",
      "None\n",
      "\n",
      " }}<|eot_id|><|start_header_id|>active<|end_header_id|>\n",
      "\n",
      "{{ Answer the question you will receive in detail, utilizing the following context.\n",
      "\n",
      "# Context:\n",
      "\n",
      "History\n",
      "\n",
      "# Question:\n",
      "\n",
      "None\n",
      "\n",
      " }}<|eot_id|><|start_header_id|>active<|end_header_id|>\n",
      "\n",
      "{{ Answer the question you will receive in detail, utilizing the following context.\n",
      "\n",
      "# Context:\n",
      "\n",
      "History\n",
      "\n",
      "# Question:\n",
      "\n",
      "None\n",
      "\n",
      " }}<|eot_id|><|start_header_id|>active<|end_header_id|>\n",
      "\n",
      "{{ Answer the question you\n"
     ]
    }
   ],
   "source": [
    "print(qa_task.reference)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompting-fb5sw-i7-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
