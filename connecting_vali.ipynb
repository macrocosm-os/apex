{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-05 10:59:13,583\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "INFO:bittensor: - Connected to test network and wss://test.finney.opentensor.ai:443/. - \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-05 10:59:30.694 |       INFO       |  - Connected to test network and wss://test.finney.opentensor.ai:443/. - \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bittensor: - Connected to test network and wss://test.finney.opentensor.ai:443/. - \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-05 10:59:33.646 |       INFO       |  - Connected to test network and wss://test.finney.opentensor.ai:443/. - \n",
      "Hotkey 5Fk35HgrTqqUffK7WN8FG4euZ8MpKx35mUYz9kgwj3UDnNHr is registered. UID: 0\n",
      "Active UIDs (total: 2): [0, 1]\n",
      "My hotkey is active: True\n",
      "Is val permit: True\n",
      "1000+ TAO:\n"
     ]
    }
   ],
   "source": [
    "# https://docs.bittensor.com/subnets/register-validate-mine#check-the-permit-status\n",
    "import bittensor as bt\n",
    "import asyncio\n",
    "from prompting.protocol import StreamPromptingSynapse\n",
    "from prompting.forward import handle_response\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# NET_UID = 61\n",
    "NET_UID = 170\n",
    "NETWORK = \"test\"\n",
    "WALLET = \"validator\"\n",
    "# WALLET = \"miner\"\n",
    "wallet = bt.wallet(name=WALLET)\n",
    "subtensor = bt.subtensor(network=NETWORK)\n",
    "subnet = subtensor.metagraph(netuid=NET_UID)\n",
    "\n",
    "hotkey = wallet.hotkey.ss58_address\n",
    "my_uid = subnet.hotkeys.index(wallet.hotkey.ss58_address)\n",
    "sub = bt.subtensor(NETWORK)\n",
    "mg = sub.metagraph(NET_UID)\n",
    "if hotkey not in mg.hotkeys:\n",
    "    print(f\"Hotkey {hotkey} deregistered\")\n",
    "else:\n",
    "    print(f\"Hotkey {hotkey} is registered. UID: {my_uid}\")\n",
    "active_uids = subnet.uids.tolist()\n",
    "print(f\"Active UIDs (total: {len(active_uids)}): {active_uids}\")\n",
    "print(f\"My hotkey is active: {my_uid in active_uids}\")\n",
    "print(f\"Is val permit: {subnet.validator_permit[my_uid]}\")\n",
    "print(f\"1000+ TAO:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:bittensor: - Response:\n",
      " The capital of Zimbabwe is Harare. - \n"
     ]
    }
   ],
   "source": [
    "uids = [my_uid]\n",
    "axons = [subnet.axons[uid] for uid in uids]\n",
    "\n",
    "# Directly call dendrite and process responses in parallel\n",
    "dendrite = bt.dendrite(wallet=wallet)\n",
    "roles = [\"user\"]\n",
    "messages = [\"Organic prompt query. Question: Capital of Zimbabwe?\"]\n",
    "timeout = 40\n",
    "model_name = \"casperhansen/llama-3-8b-instruct-awq\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "streams_responses = await dendrite(\n",
    "    axons=axons,\n",
    "    synapse=StreamPromptingSynapse(roles=roles, messages=messages),\n",
    "    timeout=timeout,\n",
    "    deserialize=False,\n",
    "    streaming=True,\n",
    ")\n",
    "stream_results_dict = dict(zip(uids, streams_responses))\n",
    "handle_stream_responses_task = asyncio.create_task(handle_response(stream_results_dict, tokenizer))\n",
    "stream_results = await handle_stream_responses_task\n",
    "\n",
    "bt.logging.info(f\"Response:\\n {stream_results[0].synapse.completion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:bittensor: - Response:\n",
      "  - \n"
     ]
    }
   ],
   "source": [
    "# Sample validators.\n",
    "# TODO: Stress test: how many queries\n",
    "\n",
    "uids = [my_uid]\n",
    "axons = [subnet.axons[uid] for uid in uids]\n",
    "\n",
    "# Directly call dendrite and process responses in parallel\n",
    "dendrite = bt.dendrite(wallet=wallet)\n",
    "roles = [\"user\"]\n",
    "messages = [\"Organic prompt query. Question: Capital of Zimbabwe?\"]\n",
    "timeout = 40\n",
    "model_name = \"casperhansen/llama-3-8b-instruct-awq\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "streams_responses = await dendrite(\n",
    "    axons=axons,\n",
    "    synapse=StreamPromptingSynapse(roles=roles, messages=messages),\n",
    "    timeout=timeout,\n",
    "    deserialize=False,\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "stream_results_dict = dict(zip(uids, streams_responses))\n",
    "handle_stream_responses_task = asyncio.create_task(handle_response(stream_results_dict, tokenizer))\n",
    "stream_results = await handle_stream_responses_task\n",
    "\n",
    "bt.logging.info(f\"Response:\\n {stream_results[0].synapse.completion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:bittensor: - Error in generating reference or handling responses for uid 0: Something went wrong with miner uid 0, Synapse is not StreamPromptingSynapse.\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/prompting-organic/prompting/forward.py\", line 74, in process_stream\n",
      "    raise ValueError(\n",
      "ValueError: Something went wrong with miner uid 0, Synapse is not StreamPromptingSynapse.\n",
      " - \n"
     ]
    }
   ],
   "source": [
    "a = await asyncio.create_task(handle_response(stream_results_dict, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SynapseStreamResult(exception=ValueError('Something went wrong with miner uid 0, Synapse is not StreamPromptingSynapse.'), uid=0, accumulated_chunks=[], accumulated_chunks_timings=[], tokens_per_chunk=[], synapse=StreamPromptingSynapse(required_hash_fields=['messages'], roles=['user'], messages=['failure'], completion=''))]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SynapseStreamResult(exception=None, uid=0, accumulated_chunks=[], accumulated_chunks_timings=[], tokens_per_chunk=[], synapse=StreamPromptingSynapse(required_hash_fields=['messages'], roles=['user'], messages=['Organic prompt query. Question: Capital of Zimbabwe?'], completion=''))]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hey Im John\n",
      "Assistant: Hi John! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# TODO: Stress test: how many queries\n",
    "from prompting.protocol import StreamPromptingSynapse\n",
    "from prompting.forward import handle_response\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "uids = [my_uid]\n",
    "axons = [subnet.axons[uid] for uid in uids]\n",
    "\n",
    "# Directly call dendrite and process responses in parallel\n",
    "dendrite = bt.dendrite(wallet=wallet)\n",
    "roles = []\n",
    "messages = []\n",
    "timeout = 40\n",
    "model_name = \"casperhansen/llama-3-8b-instruct-awq\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "while True:\n",
    "    message = input()\n",
    "    if message == \"exit\":\n",
    "        break\n",
    "    roles.append(\"user\")\n",
    "    messages.append(message)\n",
    "    streams_responses = await dendrite(\n",
    "        axons=axons,\n",
    "        synapse=StreamPromptingSynapse(roles=roles, messages=messages),\n",
    "        timeout=timeout,\n",
    "        deserialize=False,\n",
    "        streaming=True,\n",
    "    )\n",
    "    stream_results_dict = dict(zip(uids, streams_responses))\n",
    "    stream_results = await asyncio.create_task(handle_response(stream_results_dict, tokenizer))\n",
    "    response = stream_results[0].synapse.completion\n",
    "\n",
    "    print(f\"User: {message}\")\n",
    "    print(f\"Assistant: {response}\")\n",
    "\n",
    "    messages.append(response)\n",
    "    roles.append(\"assistant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_results[0].synapse.completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "async def val():\n",
    "    print(\"Val !\")\n",
    "    await asyncio.sleep(5)\n",
    "\n",
    "async def forward():\n",
    "    while True:\n",
    "        print(\"Forward\")\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "async def background_task():\n",
    "    while True:\n",
    "        print(\"Background\")\n",
    "        await val()\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "try:\n",
    "    loop.run_forever()\n",
    "except RuntimeError:\n",
    "    pass\n",
    "\n",
    "task = loop.create_task(forward())\n",
    "task2 = loop.create_task(background_task())\n",
    "try:\n",
    "    await asyncio.wait_for(task, timeout=5)\n",
    "except asyncio.TimeoutError:\n",
    "    print(\"Forward task timed out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "# Asynchronous background task\n",
    "async def background_task():\n",
    "    while True:\n",
    "        print(\"Background task is running\")\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "# Function to run the asyncio event loop in a separate thread\n",
    "def start_asyncio_loop(loop):\n",
    "    asyncio.set_event_loop(loop)\n",
    "    loop.run_forever()\n",
    "\n",
    "def main():\n",
    "    # Create a new asyncio event loop\n",
    "    loop = asyncio.new_event_loop()\n",
    "\n",
    "    # Start the event loop in a new thread\n",
    "    executor = concurrent.futures.ThreadPoolExecutor()\n",
    "    executor.submit(start_asyncio_loop, loop)\n",
    "\n",
    "    # Schedule the asynchronous background task\n",
    "    asyncio.run_coroutine_threadsafe(background_task(), loop)\n",
    "\n",
    "    # Synchronous while True loop in main\n",
    "    while True:\n",
    "        print(\"Main loop is running\")\n",
    "        time.sleep(1)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
