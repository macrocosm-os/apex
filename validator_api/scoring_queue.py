import asyncio
import datetime
from collections import deque
from typing import Any

import httpx
from loguru import logger
from pydantic import BaseModel

from shared import settings
from shared.epistula import create_header_hook
from shared.loop_runner import AsyncLoopRunner
from validator_api.validator_forwarding import ValidatorRegistry

validator_registry = ValidatorRegistry()

shared_settings = settings.shared_settings


class ScoringPayload(BaseModel):
    payload: dict[str, Any]
    retries: int = 0
    date: datetime.datetime | None = None


class ScoringQueue(AsyncLoopRunner):
    """Performs organic scoring every `interval` seconds."""

    interval: float = shared_settings.SCORING_RATE_LIMIT_SEC
    scoring_queue_threshold: int = shared_settings.SCORING_QUEUE_API_THRESHOLD
    max_scoring_retries: int = 2
    _scoring_lock = asyncio.Lock()
    _scoring_queue: deque[ScoringPayload] = deque()
    _queue_maxlen: int = 20
    _min_wait_time: float = 1

    async def wait_for_next_execution(self, last_run_time) -> datetime.datetime:
        """If scoring queue is small, execute immediately, otherwise wait until the next execution time."""
        async with self._scoring_lock:
            if self.scoring_queue_threshold < self.size > 0:
                # If scoring queue is small and non-empty, score more frequently.
                await asyncio.sleep(self._min_wait_time)
                return datetime.datetime.now()

        return await super().wait_for_next_execution(last_run_time)

    async def run_step(self):
        """Perform organic scoring: pop queued payload, forward to the validator API."""
        logger.debug("Running scoring step")
        async with self._scoring_lock:
            if not self._scoring_queue:
                return

            scoring_payload = self._scoring_queue.popleft()
            payload = scoring_payload.payload
            uids = payload["uids"]
            logger.info(
                f"Trying to score organic from {scoring_payload.date}, uids: {uids}. "
                f"Queue size: {len(self._scoring_queue)}"
            )
        try:
            vali_uid, vali_axon, vali_hotkey = validator_registry.get_available_axon()
            # get_available_axon() will return None if it cannot find an available validator, which will fail to unpack
            url = f"http://{vali_axon}/scoring"
        except Exception as e:
            logger.exception(f"Could not find available validator scoring endpoint: {e}")
        try:
            if hasattr(payload, "to_dict"):
                payload = payload.to_dict()
            elif isinstance(payload, BaseModel):
                payload = payload.model_dump()

            timeout = httpx.Timeout(timeout=120.0, connect=60.0, read=30.0, write=30.0, pool=5.0)
            # Add required headers for signature verification

            async with httpx.AsyncClient(
                timeout=timeout,
                event_hooks={"request": [create_header_hook(shared_settings.WALLET.hotkey, vali_hotkey)]},
            ) as client:
                response = await client.post(
                    url=url,
                    json=payload,
                    # headers=headers,
                )
                validator_registry.update_validators(uid=vali_uid, response_code=response.status_code)
                if response.status_code != 200:
                    # Raise an exception so that the retry logic in the except block handles it.
                    raise Exception(f"Non-200 response: {response.status_code} for uids {uids}")
                logger.debug(f"Forwarding response completed with status {response.status_code}")
        except httpx.ConnectError as e:
            logger.warning(f"Couldn't connect to validator {url} for Scoring {uids}. Exception: {e}")
        except Exception as e:
            if scoring_payload.retries < self.max_scoring_retries:
                scoring_payload.retries += 1
                async with self._scoring_lock:
                    self._scoring_queue.appendleft(scoring_payload)
                logger.warning(
                    f"Tried to forward response from {scoring_payload.date} to {url} for uids {uids}. "
                    f"Queue size: {len(self._scoring_queue)}. Exception: {e}. Queued for retry"
                )
            else:
                logger.error(
                    f"Error while forwarding response from {scoring_payload.date} after {scoring_payload.retries} "
                    f"retries: {e}"
                )
        await asyncio.sleep(0.1)

    async def append_response(
        self, uids: list[int], body: dict[str, Any], chunks: list[list[str]], timings: list[list[float]] | None = None
    ):
        if not shared_settings.SCORE_ORGANICS:
            return

        if body.get("task") != "InferenceTask" and body.get("task") != "WebRetrievalTask":
            # logger.debug(f"Skipping forwarding for non-inference/web retrieval task: {body.get('task')}")
            return

        uids = list(map(int, uids))
        chunk_dict = {str(u): c for u, c in zip(uids, chunks)}
        if timings:
            timing_dict = {str(u): t for u, t in zip(uids, timings)}
        else:
            timing_dict = {}
        payload = {"body": body, "chunks": chunk_dict, "uids": uids, "timings": timing_dict}
        scoring_item = ScoringPayload(payload=payload, date=datetime.datetime.now().replace(microsecond=0))

        async with self._scoring_lock:
            if len(self._scoring_queue) >= self._queue_maxlen:
                scoring_payload = self._scoring_queue.popleft()
                logger.info(f"Dropping oldest organic from {scoring_payload.date} for uids {uids}")
            self._scoring_queue.append(scoring_item)

    @property
    def size(self) -> int:
        return len(self._scoring_queue)

    def __len__(self) -> int:
        return self.size


# TODO: Leaving it as a global var to make less architecture changes, refactor as DI.
scoring_queue = ScoringQueue()
